{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNb5zs95r2ceiBGWN7D8jqS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ecomunick/random/blob/main/scraping_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pAVhOjK9Dl8s"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# tutorial from here: https://scrapfly.io/blog/web-scraping-with-python-beautifulsoup/#how-is-html-parsed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "html = \"\"\"\n",
        "<div class=\"product\">\n",
        "  <h2>Product Title</h2>\n",
        "  <div class=\"price\">\n",
        "    <span class=\"discount\">12.99</span>\n",
        "    <span class=\"full\">19.99</span>\n",
        "  </div>\n",
        "</div>\n",
        "\"\"\"\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "soup = BeautifulSoup(html)\n",
        "product = {\n",
        "    \"title\": soup.find(class_=\"product\").find(\"h2\").text,\n",
        "    \"full_price\": soup.find(class_=\"product\").find(class_=\"full\").text,\n",
        "    \"price\": soup.select_one(\".price .discount\").text,\n",
        "}\n",
        "print(product)\n",
        "{\n",
        "    \"title\": \"Product Title\",\n",
        "    \"full_price\": \"19.99\",\n",
        "    \"price\": \"12.99\",\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PrrgjshEjGW",
        "outputId": "cc4a1895-9ef8-41f9-e653-4753dccb0a07"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'title': 'Product Title', 'full_price': '19.99', 'price': '12.99'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'Product Title', 'full_price': '19.99', 'price': '12.99'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is our HTML page:\n",
        "html = \"\"\"\n",
        "<head>\n",
        "  <title class=\"page-title\">Hello World!</title>\n",
        "</head>\n",
        "<body>\n",
        "  <div id=\"content\">\n",
        "    <h1>Title</h1>\n",
        "    <p>first paragraph</p>\n",
        "    <p>second paragraph</p>\n",
        "    <h2>Subtitle</h2>\n",
        "    <p>first paragraph of subtitle</p>\n",
        "  </div>\n",
        "</body>\n",
        "\"\"\"\n",
        "\n",
        "# 1. build soup object from html text\n",
        "soup = BeautifulSoup(html, 'lxml')\n",
        "\n",
        "# then we can navigate the html tree via python API:\n",
        "# for example title is under `head` node:\n",
        "print(soup.head.title)\n",
        "'<title class=\"page-title\">Hello World!</title>'\n",
        "\n",
        "# this gives us a whole HTML node but we can also just select the text:\n",
        "print(soup.head.title.text)\n",
        "\"Hello World!\"\n",
        "\n",
        "# or it's other attributes:\n",
        "print(soup.head.title[\"class\"])\n",
        "\"page-title\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "CH-IVHLWFod6",
        "outputId": "9a001c23-c987-48e2-d515-928213aacfe0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<title class=\"page-title\">Hello World!</title>\n",
            "Hello World!\n",
            "['page-title']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'page-title'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#soup.body.div.div.div.p.a['href']\n",
        "\n",
        "if soup.body and soup.body.div and soup.body.div.div and soup.body.div.div.div and soup.body.div.div.div.p and soup.body.div.div.div.p.a:\n",
        "    href = soup.body.div.div.div.p.a['href']\n",
        "else:\n",
        "    href = None  # or handle the case when the element doesn't exist\n"
      ],
      "metadata": {
        "id": "az2lbPFvHA6s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(soup)  # Print the entire soup object to inspect the parsed HTML\n",
        "\n",
        "# Print specific elements to check their existence and structure\n",
        "if soup.body:\n",
        "    print(soup.body)\n",
        "    if soup.body.div:\n",
        "        print(soup.body.div)\n",
        "        if soup.body.div.div:\n",
        "            print(soup.body.div.div)\n",
        "            if soup.body.div.div.div:\n",
        "                print(soup.body.div.div.div)\n",
        "                if soup.body.div.div.div.p:\n",
        "                    print(soup.body.div.div.div.p)\n",
        "                    if soup.body.div.div.div.p.a:\n",
        "                        print(soup.body.div.div.div.p.a['href'])\n",
        "                    else:\n",
        "                        print(\"The 'a' element is missing.\")\n",
        "                else:\n",
        "                    print(\"The 'p' element is missing.\")\n",
        "            else:\n",
        "                print(\"The third 'div' element is missing.\")\n",
        "        else:\n",
        "            print(\"The second 'div' element is missing.\")\n",
        "    else:\n",
        "        print(\"The first 'div' element is missing.\")\n",
        "else:\n",
        "    print(\"The 'body' element is missing.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8fRPNP9HULl",
        "outputId": "9346a4fe-6253-4023-93cb-72ac830dbf42"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<html><head>\n",
            "<title class=\"page-title\">Hello World!</title>\n",
            "</head>\n",
            "<body>\n",
            "<div id=\"content\">\n",
            "<h1>Title</h1>\n",
            "<p>first paragraph</p>\n",
            "<p>second paragraph</p>\n",
            "<h2>Subtitle</h2>\n",
            "<p>first paragraph of subtitle</p>\n",
            "</div>\n",
            "</body>\n",
            "</html>\n",
            "<body>\n",
            "<div id=\"content\">\n",
            "<h1>Title</h1>\n",
            "<p>first paragraph</p>\n",
            "<p>second paragraph</p>\n",
            "<h2>Subtitle</h2>\n",
            "<p>first paragraph of subtitle</p>\n",
            "</div>\n",
            "</body>\n",
            "<div id=\"content\">\n",
            "<h1>Title</h1>\n",
            "<p>first paragraph</p>\n",
            "<p>second paragraph</p>\n",
            "<h2>Subtitle</h2>\n",
            "<p>first paragraph of subtitle</p>\n",
            "</div>\n",
            "The second 'div' element is missing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = \"\"\"\n",
        "<head>\n",
        "  <title class=\"page-title\">Hello World!</title>\n",
        "</head>\n",
        "<body>\n",
        "  <div id=\"content\">\n",
        "    <h1>Title</h1>\n",
        "    <p>first paragraph</p>\n",
        "    <p>second paragraph</p>\n",
        "    <h2>Subtitle</h2>\n",
        "    <p>first paragraph of subtitle</p>\n",
        "  </div>\n",
        "</body>\n",
        "\"\"\"\n",
        "\n",
        "soup = BeautifulSoup(html, 'lxml')\n",
        "\n",
        "print(soup.find('title').text)\n",
        "# Output: Hello World!\n",
        "\n",
        "print(soup.find(class_='page-title').text)\n",
        "# Output: Hello World!\n",
        "\n",
        "print(soup.find('div', id='content').h2.text)\n",
        "# Output: Subtitle\n",
        "\n",
        "print(soup.find_all('p', text=re.compile('first')))\n",
        "# Output: [\"<p>first paragraph</p>\", \"<p>first paragraph of subtitle</p>\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVWVr4cPII7h",
        "outputId": "e7e62825-9fd2-4e19-d83b-a2b213c14d7f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World!\n",
            "Hello World!\n",
            "Subtitle\n",
            "[<p>first paragraph</p>, <p>first paragraph of subtitle</p>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-6c15b0619930>:30: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  print(soup.find_all('p', text=re.compile('first')))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = \"\"\"\n",
        "<head>\n",
        "  <title class=\"page-title\">Hello World!</title>\n",
        "</head>\n",
        "<body>\n",
        "  <div id=\"content\">\n",
        "    <h1>Title</h1>\n",
        "    <p>first paragraph</p>\n",
        "    <p>second paragraph</p>\n",
        "    <h2>Subtitle</h2>\n",
        "    <p>first paragraph of subtitle</p>\n",
        "  </div>\n",
        "</body>\n",
        "\"\"\"\n",
        "soup = BeautifulSoup(html, 'lxml')\n",
        "\n",
        "soup.select_one('title').text\n",
        "\"Hello World\"\n",
        "\n",
        "# we can also perform searching by attribute values such as class names\n",
        "soup.select_one('.page-title').text\n",
        "\"Hello World\"\n",
        "\n",
        "# We can also find _all_ amtching values:\n",
        "for paragraph in soup.select('#content p'):\n",
        "    print(paragraph.text)\n",
        "\"first paragraph\"\n",
        "\"second paragraph\"\n",
        "\"first paragraph of subtitile\"\n",
        "\n",
        "# We can also combine CSS selectors with find functions:\n",
        "import re\n",
        "# select node with id=content and then find all paragraphs with text \"first\" that are under it:\n",
        "soup.select_one('#content').find_all('p', text=re.compile('first'))\n",
        "[\"<p>first paragraph</p>\", \"<p>first paragraph of subtitle</p>\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7itWTkM3IeT0",
        "outputId": "bf9b8098-9f71-409e-ae65-5a071d181361"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first paragraph\n",
            "second paragraph\n",
            "first paragraph of subtitle\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-167cfc05ef9e>:36: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  soup.select_one('#content').find_all('p', text=re.compile('first'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<p>first paragraph</p>', '<p>first paragraph of subtitle</p>']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = \"\"\"\n",
        "<div>\n",
        "  <a>The Avangers: </a>\n",
        "  <a>End Game</a>\n",
        "  <p>is one of the most popular Marvel movies</p>\n",
        "</div>\n",
        "\"\"\"\n",
        "soup = BeautifulSoup(html, 'lxml')\n",
        "# join all text values with space, and strip leading/trailing whitespace:\n",
        "soup.div.get_text(' ', strip=True)\n",
        "'The Avangers: End Game is one of the most popular Marvel movies'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mPLWLQuXJz74",
        "outputId": "1748d04d-3649-423f-b63e-d69a5f3c6121"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Avangers: End Game is one of the most popular Marvel movies'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = \"\"\"\n",
        "<div><h1>The Avangers: </h1><a>End Game</a><p>is one of the most popular Marvel movies</p></div>\n",
        "\"\"\"\n",
        "soup = BeautifulSoup(html)\n",
        "soup.prettify()\n",
        "\"\"\"\n",
        "<html>\n",
        " <body>\n",
        "  <div>\n",
        "   <h1>\n",
        "    The Avangers:\n",
        "   </h1>\n",
        "   <a>\n",
        "    End Game\n",
        "   </a>\n",
        "   <p>\n",
        "    is one of the most popular Marvel movies\n",
        "   </p>\n",
        "  </div>\n",
        " </body>\n",
        "</html>\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "m1SLRcp3KBkS",
        "outputId": "b4e3b501-8f6c-402b-bcd6-5115695f45e6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n<html>\\n <body>\\n  <div>\\n   <h1>\\n    The Avangers:\\n   </h1>\\n   <a>\\n    End Game\\n   </a>\\n   <p>\\n    is one of the most popular Marvel movies\\n   </p>\\n  </div>\\n </body>\\n</html>\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup, SoupStrainer\n",
        "html = \"\"\"\n",
        "<head><title>hello world</title></head>\n",
        "<body>\n",
        "  <div>\n",
        "      <a>Link 1</a>\n",
        "      <a>Link 2</a>\n",
        "      <div>\n",
        "        <a>Link 3</a>\n",
        "      /div>\n",
        "  </div>\n",
        "</body>\n",
        "\"\"\"\n",
        "link_strainer = SoupStrainer('a')\n",
        "soup = BeautifulSoup(html, parse_only=link_strainer)\n",
        "print(soup)\n",
        "#<a>Link 1</a><a>Link 2</a><a>Link 3</a>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR15UBK5KO39",
        "outputId": "b90de5fc-e22b-4bce-b7c8-6d81262a108b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<a>Link 1</a><a>Link 2</a><a>Link 3</a>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "html = \"\"\"\n",
        "<div>\n",
        "  <button class=\"flat-button red\">Subscribe</button>\n",
        "</div>\n",
        "\"\"\"\n",
        "soup = BeautifulSoup(html)\n",
        "soup.div.button['class'] = \"shiny-button blue\"\n",
        "soup.div.button.string = \"Unsubscribe\"\n",
        "print(soup.prettify())\n",
        "# <html>\n",
        "#  <body>\n",
        "#   <div>\n",
        "#    <button class=\"shiny-button blue\">\n",
        "#     Unsubscribe\n",
        "#    </button>\n",
        "#   </div>\n",
        "#  </body>\n",
        "# </html>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TomWEm38KZUr",
        "outputId": "3d7474b6-ae88-48b8-d72e-0cba483be479"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<html>\n",
            " <body>\n",
            "  <div>\n",
            "   <button class=\"shiny-button blue\">\n",
            "    Unsubscribe\n",
            "   </button>\n",
            "  </div>\n",
            " </body>\n",
            "</html>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iWq4i0a0KibC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scrapfly-sdk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4P0_3qzL3Jz",
        "outputId": "14da7e8b-71ce-444b-ad8d-e5766559f478"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scrapfly-sdk\n",
            "  Downloading scrapfly_sdk-0.8.8-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: decorator>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from scrapfly-sdk) (4.4.2)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from scrapfly-sdk) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from scrapfly-sdk) (2.8.2)\n",
            "Collecting loguru>=0.5 (from scrapfly-sdk)\n",
            "  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from scrapfly-sdk) (2.0.4)\n",
            "Collecting backoff>=1.10.0 (from scrapfly-sdk)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->scrapfly-sdk) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->scrapfly-sdk) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->scrapfly-sdk) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->scrapfly-sdk) (2023.7.22)\n",
            "Installing collected packages: loguru, backoff, scrapfly-sdk\n",
            "Successfully installed backoff-2.2.1 loguru-0.7.0 scrapfly-sdk-0.8.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "from urllib.parse import urljoin\n",
        "from scrapfly import ScrapflyClient, ScrapeConfig, ScrapeApiResponse\n",
        "\n",
        "url = \"https://www.remotepython.com/jobs/\"\n",
        "client = ScrapflyClient(key=\"YOUR SCRAPFLY KEY\")\n",
        "result: ScrapeApiResponse = client.scrape(ScrapeConfig(\n",
        "    url=url,\n",
        "    # we can select specific country:\n",
        "    country=\"US\",\n",
        "    # proxy type:\n",
        "    proxy_pool=\"public_residential_pool\",\n",
        "    # we can also enable headless browser powered js rendering\n",
        "    render_js=True,\n",
        "))\n",
        "# scrapfly SDK comes with beautifulsoup built-in:\n",
        "job_listing_boxes = result.soup.find_all(class_=\"item\")\n",
        "\n",
        "results = []\n",
        "for item in job_listing_boxes:\n",
        "    parsed = {}\n",
        "    if title := item.find(\"h3\"):\n",
        "        parsed[\"title\"] = title.get_text(strip=True)\n",
        "    if item_url := item.find(\"h3\").a[\"href\"]:\n",
        "        parsed[\"url\"] = urljoin(url, item_url)\n",
        "    if company := item.find(\"h5\").find(\"span\", class_=\"color-black\"):\n",
        "        parsed[\"company\"] = company.text\n",
        "    if location := item.select_one(\"h5 .color-white-mute\"):\n",
        "        parsed[\"location\"] = location.text\n",
        "    if posted_on := item.find(\"span\", class_=\"color-white-mute\", text=re.compile(\"posted:\", re.I)):\n",
        "        parsed[\"posted_on\"] = posted_on.text.split(\"Posted:\")[-1].strip()\n",
        "    results.append(parsed)\n",
        "\n",
        "print(results)\n",
        "[{\n",
        "    \"title\": \"Hiring Senior Python / DJANGO Developer\",\n",
        "    \"url\": \"https://www.remotepython.com/jobs/3edf4109d642494d81719fc9fe8dd5d6/\",\n",
        "    \"company\": \"Mathieu Holding sarl\",\n",
        "    \"location\": \"Rennes, France\",\n",
        "    \"posted_on\": \"Sept. 1, 2022\"\n",
        "  },\n",
        "  ...\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "BugDRACoL6w9",
        "outputId": "92170543-8d12-4392-bb73-5598701ab094"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "EncoderError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mExtraData\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scrapfly/api_response.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, content)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmsgpack/_unpacker.pyx\u001b[0m in \u001b[0;36mmsgpack._cmsgpack.unpackb\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mExtraData\u001b[0m: unpack(b) received extra data.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mEncoderError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-8dffde453aa7>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://www.remotepython.com/jobs/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScrapflyClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"YOUR SCRAPFLY KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m result: ScrapeApiResponse = client.scrape(ScrapeConfig(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# we can select specific country:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/backoff/_sync.py\u001b[0m in \u001b[0;36mretry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mexception\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mmax_tries_exceeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtries\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_tries_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scrapfly/client.py\u001b[0m in \u001b[0;36mscrape\u001b[0;34m(self, scrape_config)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscrape_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mScrapeConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mScrapeApiResponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scrapfly/client.py\u001b[0m in \u001b[0;36mscrape\u001b[0;34m(self, scrape_config)\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mrequest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scrape_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrape_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscrape_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_http_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mscrape_api_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscrape_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscrape_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrape_api_response\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscrape_api_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scrapfly/client.py\u001b[0m in \u001b[0;36m_handle_response\u001b[0;34m(self, response, scrape_config)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscrape_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mScrapeConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mScrapeApiResponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             api_response = self._handle_api_response(\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0mscrape_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscrape_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scrapfly/client.py\u001b[0m in \u001b[0;36m_handle_api_response\u001b[0;34m(self, response, scrape_config, raise_on_upstream_error)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m                 \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scrapfly/api_response.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, content)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEncoderError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEncoderError\u001b[0m: {\"error_id\":\"477db6e5-56e8-4dff-a65c-4c6260bdb908\",\"http_code\":401,\"message\":\"Invalid API key\",\"reason\":\"Unauthorized\"}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BNiCWnaHMRrW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}